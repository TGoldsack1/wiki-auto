{"index": 100, "ground_truth": "beat: asynchronous bft made practical duan et al., ccs\u201918 reaching agreement (consensus) is hard enough, doing it in the presence of active adversaries who can tamper with or destroy your communications is much harder still. that\u2019s the world of byzantine fault tolerance (bft). we\u2019ve looked at practical bft (pbft) and honeybadger on previous editions of the morning paper. today\u2019s paper, beat, builds on top of honeybadger to offer bft with even better latency and throughput. asynchronous bft protocols are arguably the most appropriate solutions for building high-assurance and intrusion-tolerant permissioned blockchains in wide-are (wan) environments, as these asynchronous protocols", "prediction": "asynchronous environments. and extensible, consisting of five asynchronous protocols that are designed to meet different goals (e.g., different performance scenarios). due to modularity in its design, features of these protocols can be mixed to achieve even more meaningful trade-offs between functionality and performance for various applications. through a 92-instance, five-continent deployment of amazon ec2, we show that beat on amazon ec2, all our beat instances significantly outperform, in terms of both latency .", "simplified_prediction": "It environments . and extensible , which is made of five protocols that are designed to meet different goals ( e. g. , different performance scenarios ) . This is because modularity in its design , features of these protocols can be mixed to achieve even more meaningful trade between functionality and performance for various applications . through a 92 - instance , five use of amazon ec2 , we show that beat on amazon ec2 , all our beat instances significantly outperform , in terms of both latency ."}
{"index": 101, "ground_truth": "this paper presents an integrated behavioral anticipation and decision-making system that models behavior for both our vehicle and nearby vehicles as the result of closed-loop policies. only a finite set of a priori known policies are considered. bayesian changepoint detection is used to estimate which policy a given vehicle was executing at each point in its history of actions, then infer the likelihood of each potential intention of the vehicle. a statistical test is proposed based on changepoint detection to identify anomalous behavior of other vehicles, such as driving in the wrong direction or swerving out of lanes. evidence", "prediction": "paper presents an integrated behavioral inference and decision-making approach that models vehicle behavior for both our vehicle and nearby vehicles as a discrete set of closedloop policies that react to the actions of other agents. each policy captures a distinct high-level behavior and intention, such as driving along a lane or turning at an intersection. we first employ bayesian changepoint detection on the observed history of states of nearby cars to estimate the distribution over potential policies that each nearby car must evaluate the consequences of its potential actions by anticipating the uncertain intentions of other traffic participants. this work extends our previous multipolicy system [11] by incorporating behavioral anticipation into decision-making to evaluate sampled potential vehicle interactions. .", "simplified_prediction": "Inference and decision-making approach that models vehicle behavior for both our vehicle and nearby vehicles as a set of closedloop policies that react to the actions of other agents to stop them . Each policy captures a distinct '' high-level behavior '' behavior , such as driving along a lane or turning at an area . we employ bayesian changepoint detection on the observed history of states of nearby cars to estimate the distribution over potential policies that each nearby car must evaluate the consequences of its potential actions . This may be because of the uncertain intentions of other traffic participants . This work extends our previous multipolicy system [ 11 ] by using behavioral anticipation into decision-making to evaluate potential vehicle interactions . Other pages"}
{"index": 102, "ground_truth": "the paper presents the task of abductive nlp (pronounced as alpha nlp) where the model needs to perform abductive reasoning. abductive reasoning is the inference to the most plausible explanation. even though it is considered to be an important component for understanding narratives, the work in this domain is sparse. a new dataset called as abstractive reasoning in narrative text (art) consisting of 20k narrative contexts and 200k explanations is also provided. the dataset models the task as multiple-choice questions to make the evaluation process easy. task setup given a pair of observations o1 and o2 and two hypothesis", "prediction": "abductive abductive reasoning. has been considered to be at the core of how people interpret and read between the lines in natural language inference and generation. we present the first study that investigates the viability of language-based abductive we introduce a challenge dataset, art, that consists of over 20k commonsense narrative contexts and 200k explanations. based on this dataset, we conceptualize two new tasks \u2013 (i) abductive a conditional generation task for explaining given observations in natural language. on abductive nli, the current best language generators struggle even more, as they lack reasoning capabilities that are trivial for humans. our analysis leads to new insights .", "simplified_prediction": "Abductive abductive reasoning . It has been thought to be at the core of how people interpret and read between the lines in natural language and generation . we present the first study that looks at language-based abductive we introduce a challenge dataset , art , that has over 20k commonsense narrative contexts and 200k explanations . based on this dataset , we think of two new tasks ( i ) abductive a way of explaining what a way to explain about it in natural language . The current best language generators struggle even more , as they do not have a lot of features that are trivial for humans . our analysis leads to new groups ."}
{"index": 103, "ground_truth": "`update 2015/11/23: since i first wrote this note, i became involved in the next iterations of this work, which became v2 of the arxiv manuscript. the notes below were made based on v1.` this paper considers the problem of maximum inner product search (mips). in mips, given a query $q$ and a set of inputs $x_i$, we want to find the input (or the top n inputs) with highest inner product, i.e. $argmax_i q' x_i$. recently, it was shown that a simple transformation to the query and input vectors made it possible to approximately solve mips using hashing methods", "prediction": "search (mips) is an important task that has a wide applicability in recommendation systems and classification with a large number of classes. solutions based on locality-sensitive (lsh) as well as tree-based solutions have been investigated in the recent literature, to perform approximate mips in sublinear time. in this paper, we compare these to another extremely simple approach for solving approximate mips, based on two standard recommendation system benchmarks as well as on large vocabulary word embeddings, show that this simple approach yields much higher speedups, for the same retrieval precision, than current state-of-the-art hashing-based and tree-based methods. this simple method also yields more robust retrievals when the query is corrupted by noise. the maximum inner product search (mips) problem has recently received increased attention, as it arises naturally in many large scale tasks. .", "simplified_prediction": "The search is an important task that has a wide applicability in recommendation systems and classification with a large number of classes . The solutions based on locality-sensitive ( lsh ) as well as tree-based solutions have been investigated in the recent literature , for a long time . In this paper , we compare these to another extremely simple approach for solving mips , based on two standard recommendation system benchmarks as well as on large vocabulary word embeddings , show that this simple approach yields much higher speedups , for the same retrieval precision , than current state-of-the-art hashing and tree-based methods . This method also gives more robust find out when the query is corrupted by noise . The maximum inner product search ( mips ) problem has recently received increased attention , because many large tasks have been found . Other pages"}
{"index": 104, "ground_truth": "understanding lifecycle management complexity of datacenter topologies zhang et al., nsdi\u201919 there has been plenty of interesting research on network topologies for datacenters, with clos-like tree topologies and expander based graph topologies both shown to scale using widely deployed hardware. this research tends to focus on performance properties such as throughput and latency, together with resilience to failures. important as these are, note that they\u2019re also what\u2019s right in front of you as a designer, and relatively easy to measure. the great thing about today\u2019s paper is that the authors look beneath the surface to consider the less visible", "prediction": "most recent designs have focused on performance properties such as latency and throughput. in this paper, we devise complexity metrics for lifecycle management, and show that existing topology classes have low lifecycle management complexity by some measures, but not by others. motivated by this, we design a new class of topologies, fatclique, that, while being performance-equivalent to existing topologies, is comparable to, or better than them by all our lifecycle management complexity metrics. over the past decade, there has been a long line of work on designing datacenter topologies 35, 31, management [30, management .", "simplified_prediction": "Most recent designs have focused on performance properties such as latency and throughput . In this paper , we show that existing topology classes have low lifecycle management complexity by some measures , but not by others , but not by other people . We design a new class of topologies , fatclique , that , while being performance-equivalent to existing topologies , is similar to , or better than them by all our lifecycle management complexity . There has been a long line of work on designing datacenter topologies 35 , 31 , management ( 30 , management ) ."}
{"index": 105, "ground_truth": "kv-direct: high-performance in-memory key-value store with programmable nic li et al., sosp\u201917 we\u2019ve seen some pretty impressive in-memory datastores in past editions of the morning paper, including farm , ramcloud , and drtm . but nothing that compares with kv-direct: with 10 programmable nic cards in a commodity server, we achieve 1.22 billion kv operations per second, which is almost an order-of-magnitude improvement over existing systems, setting a new milestone for a general-purpose in-memory key-value store. check out the bottom line in this comparison table from the evaluation: ( enlarge ) in addition to sheer speed,", "prediction": "modern kvs goes beyond the traditional object-caching workload . data centers, shifting the bottleneck of most kvs from the network to the cpu. rdma-capable nic partly alleviates the problem, but the primitives provided by rdma abstraction are rather limited. meanwhile, programmable in data centers. recent years have witnessed a rapid increase of network bandwidth in data centers, recent years .", "simplified_prediction": "modern kvs goes beyond the traditional object-caching workload . data centers , changed the bottleneck of most kvs from the network to the cpu . However , the primitives provided by rdma abstraction are rather limited , but the primitives provided by rdma abstraction . After this , programmable in data centers . recent years have saw a rapid increase of network bandwidth in data centers , recent years ."}
{"index": 106, "ground_truth": "the paper presents a generalized framework for graph clustering (clusters of network motifs) on the basis of higher-order connectivity patterns. approach given a motif m , the framework aims to find a cluster of the set of nodes s such that nodes of s participate in many instances of m and avoid cutting instances of m (that is only a subset of nodes in instances of m appears in s). mathematically, the aim is to minimise the motif conductance metric given as cutm(s, s\u2019) / min[volm(s), volm(s\u2019)] where s\u2019 is complement of s, cutm(s, s\u2019) = number of instances", "prediction": "networks are a fundamental tool for understanding and modeling complex systems in physics, biology, and social science. many networks are known to exhibit rich, lower-order connectivity patterns that can be captured at the level of individual nodes and edges. however, organization of complex networks\u2014at the level of small network subgraphs\u2014 remains largely unknown. here we develop a generalized framework for clustering networks based on higher-order connectivity patterns. this framework provides mathematical guarantees on the optimality of obtained clusters and scales to networks with billions of edges. the framework reveals higher-order organization in a number of networks including information propagation units in neuronal networks and hub structure in transportation networks. results show that networks exhibit rich higher-order organizational structures that are exposed by clustering based on higher-order connectivity patterns. organization of complex networks .", "simplified_prediction": "networks are important for understanding and modeling systems in physics , biology , and social science . Many networks have rich , lower-order connectivity patterns that can be captured at the level of individual nodes and edges . However , organization of complex networks changed the level of small network subgraphs that are not known . we develop a good framework for clustering networks , based on higher-order patterns . This means that mathematical guarantees on the optimality of obtained clusters and scales to networks with billions of edges . The framework shows higher-order organization in a number of networks including information propagation units that can be used in transportation networks . The networks show rich higher-order structures that are exposed by clusters . These structures are based on higher-order connectivity patterns . It is based on complex networks ."}
{"index": 107, "ground_truth": "they suggest a new method to train gans. they start training them at low resolution (4x4), wait until \"convergence\", then add more convolutions to the existing model to generate and discriminate higher resolutions. each new block of convolutions is slowly blended in, instead of being added from one batch to the next. combined with two new normalization techniques, they get good-looking images at up to 1024x1024 on their new celeba-hq dataset (celeba in high resolution). they also suggest a new scoring method based on the approximated wasserstein distance between real and generated image patches. according to that score, their progressive", "prediction": "new layers that model increasingly fine details as training progresses. this both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., images at 1024. we also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised cifar10. additionally, et al., et al., et al., 2017; et al., 2017; et al., 2017; et al., et .", "simplified_prediction": "new layers that model increasingly fine details , like training . This speeds the training up and greatly stabilizes it , allowing us to produce images of good quality , e.g. , images at 1024 ; and a simple way to increase the variation in generated images , and achieve a record inception score of 8.80 in unsupervised cifar10 . et al . , et al . , et al . , et al . , 2017 ; et al . , 2018 ; et al . , et al . , et al ."}
{"index": 108, "ground_truth": "in this paper, the authors proposed a method for convexifying convolutional neural networks to train them without backpropagation. furthermore, this relaxation to the convex setting allows for theoretical proofs of bounds on the generalization error. succinctly, they propose to use rkhs and the kernel trick to lift the data into a high-dimensional space that is expressive enough to capture certain nonlinear activation functions. hence, on experiments on mnist and cifar-10, they show that they can outperform smaller cnns by \u201cconvexifying\u201d them. they note that their method doesn\u2019t work with max pooling or very deep cnns with lots of bells and", "prediction": "convexified convolutional neural networks have proven successful across many tasks in machine learning and artificial intelligence, including image classification [28, 25], face recognition [26], speech recognition [21], text classification [45], and game playing [32, 37]. as the actual performance is determined by some combination of the cnn architecture along with the optimization algorithm. .", "simplified_prediction": "They have proven successful across many tasks in machine learning and artificial intelligence , including image classification [ 28 , 25 ] , face recognition [ 26 ] , speech recognition [ 21 ] , text classification [ 45 ] , and game playing [ 32 , 37 ] . As the actual performance is determined by some combination of the cnn architecture along with the algorithms using it . Other pages"}
{"index": 109, "ground_truth": "proposes a novel, end-to-end architecture for generating short email responses. single most important benchmark of its success is that it is deployed in inbox by gmail and assists with around 10% of all mobile responses. . challenges in deploying smart reply in a user-facing product responses must always be of high quality. ensured by constructing a target response set to select responses from. the likelihood of choosing the responses must be maximised. ensured by normalising the responses and enforcing diversity. the system should not add latency to emails. ensured by using a triggering model to decide if the email", "prediction": "the development of intelligent machines is one of the biggest unsolved challenges in computer science. in the last decades the computational community has preferred to focus on solving relatively narrow empirical problems that are important for specific applications, but do not address the overarching goal of developing general-purpose communication, as a prerequisite to be pursued all at once, in the last article, we propose an alternative approach: from performing menial jobs for us to helping the advancement of basic and applied research. given the current availability of powerful hardware and large amounts of machine-readable data, as well as the widespread interest in sophisticated machine learning methods, the times should be ripe for the development of intelligent machines. still, .", "simplified_prediction": "The development of intelligent machines is one of the biggest challenges in computer science . In the last decades the computational community has preferred to focus on solving relatively narrow empirical problems that are important for specific applications , but do not address the overarching goal of developing general-purpose communication , as a prerequisite to be done all at once , in the last article , we propose an alternative approach : from performing menial jobs for us to helping the advance of basic and applied research . However , the current availability of powerful hardware and large amounts of machine-readable data , as well as the widespread interest in machine learning methods , the times should be ripe for the development of intelligent machines . and still ."}
{"index": 110, "ground_truth": "build a supervised reading comprehension data set using news corpus. compare the performance of neural models and state-of-the-art natural language processing model on reading comprehension task. reading comprehension estimate conditional probability p(a|c, q), where c is a context document, q is a query related to the document, and a is the answer to that query. dataset generation use online newspapers (cnn and dailymail) and their matching summaries. parse summaries and bullet points into cloze style questions. generate corpus of document-query-answer triplets by replacing one entity at a time with a placeholder. data anonymized and randomised using coreference systems,", "prediction": "large scale training and test datasets have been missing for this type of evaluation. in this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. this allows us to develop a class of attention based deep neural networks that learn to read real documents and understanding documents that they have seen, but until now large scale training and test datasets have been based on either hand engineered grammars [1], or information extraction methods of detecting predicate argument triples that can later be queried as a relational database [2]. supervised machine learning approaches have largely been absent from this space due to both the lack of large scale training datasets, .", "simplified_prediction": "large scale training and test datasets have been missing for this type of work . In this work we define a new methodology that shows this bottleneck and provides large scale supervised reading data . This allows us to develop a class of attention based deep neural networks that learn to read real documents and understanding documents that they have seen , but now large scale training and test datasets have been based on either hand engineered grammars [ 1 ] , or information that can later be used as a relation to find a database . supervised machine learning approaches have largely been absent from this space because they do not have a lot of large scale training data ."}
{"index": 111, "ground_truth": "rpcvalet: ni-driven tail-aware balancing of \u00b5s-scale rpcs daglis et al., asplos\u201919 last week we learned about the [increased tail-latency sensitivity of microservices based applications with high rpc fan-outs. seer uses estimates of queue depths to mitigate latency spikes on the order of 10-100ms, in conjunction with a cluster manager. today\u2019s paper choice, rpcvalet, operates at latencies 3 orders of magnitude lower, targeting reduction in tail latency for services that themselves have service times on the order of a small number of \u00b5s (e.g., the average service time for memcached is approximately 2\u00b5s). the net result of rapid advancements in", "prediction": "modern online services come with stringent quality requirements in terms of response time tail latency. because of their decomposition into fine-grained communicating software layers, a single user request fans out into a plethora of short, \u03bcs-scale aggravating the need for faster inter-server communication. in reaction to that need, we are witnessing a technological transition characterized by the emergence of hardware-terminated user-level and new architectures with fully integrated network interfaces (nis). such architectures offer a unique opportunity for a new ni-driven approach to balancing rpcs among the cores of manycore server cpus, yielding major tail latency improvements .", "simplified_prediction": "modern online services come with quality requirements in terms of response time tail latency . Because of their decomposition into good-grained communicating software layers , a single user request fans out into a plethora of short , making the need for faster inter-server communication . In reaction to that need , we are witnessing a change of hardware-terminated user-level and new architectures with fully integrated network interfaces ( called '' nis '' ) . such architectures offer a opportunity for a new ni-driven approach to balancing rpcs among the cores of manycore server cpus . They also have major tail latency improvements ."}
{"index": 112, "ground_truth": "scalable atomic visibility with ramp transactions \u2013 bailis et al. 2014 ramp transactions came up last week as part of the secret sauce in coordination avoidance in database systems that contributed to a 25x improvement on the tpc-c benchmark. so what exactly are ramp transactions and why might we need them? as soon as you partition your database across multiple servers, things start to get interesting. we\u2019d like to maintain atomic isolation \u2013 either all of a transaction\u2019s effects are visible or none are \u2013 for transactions that span partitions\u2026 the status quo for these multi-partition atomic transactions", "prediction": "databases can provide scalability by ensuring atomic visibility: either or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. copyrights for components that tolerate these communication delays is a difficult task but is key to maintaining scalability [17, 29, 35]. in this work, we address a largely underserved class of applications requiring near-unlimited operations. .", "simplified_prediction": "A database can also mean : either or part of this work for personal or classroom use is granted without fee . This gives that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page . The copyrights for parts that tolerate these communication delays is a difficult task but is very difficult to keep scalability [ 17 , 29 , 35 . In this work , we address a largely underserved class of applications which needed to work . Other pages"}
{"index": 113, "ground_truth": "uncertainty propagation in data processing systems manousakis et al., socc\u201918 when i\u2019m writing an edition of the morning paper, i often imagine a conversation with a hypothetical reader sat in a coffee shop somewhere at the start of their day. there are three levels of takeaway from today\u2019s paper choice: if you\u2019re downing a quick espresso, then it\u2019s good to know that uncertainty can creep into our data in lots of different ways, and if you compute with those uncertain values as if they were precise, errors can compound quickly leading to incorrect results or false confidence. if", "prediction": "results. unfortunately, and machine learning algorithms. performing computations on uncertain data as if it were exact leads to incorrect results. sensors in iot, sampling-based approximate computations and use it to modify ten applications, including ai/ml, image processing and trend analysis applications to be expressed as directed acyclic graphs (dags) of side-effect free computation nodes, with data flowing through the edges for processing. the frameworks then run applications on clusters of servers, transparently handling issues such as task scheduling, data .", "simplified_prediction": "results . It is also called machine learning algorithms . performing computations on uncertain data as if it was not exact to incorrect results . The sensors in iot , sampling-based approximate computations and use it to modify ten applications , including ai / ml , image processing and trend analysis applications to be expressed as directed ( dags ) of free computation nodes , with data flowing through the edges for processing . The frameworks then run applications on clusters of servers , handling issues such as task scheduling , data and data ."}
{"index": 114, "ground_truth": "mining high-speed data streams \u2013 domingos & hulten 2000 this paper won a \u2018test of time\u2019 award at kdd\u201915 as an \u2018outstanding paper from a past kdd conference beyond the last decade that has had an important impact on the data mining community.\u2019 here\u2019s what the test-of-time committee have to say about it: this paper proposes a decision tree learner for data streams, the hoeffding tree algorithm, which comes with the guarantee that the learned decision tree is asymptotically nearly identical to that of a non-incremental learner using infinitely many examples. this work constitutes a significant step", "prediction": "many organizations today have more than very large databases; they have databases that grow without limit at a rate of several million records per day. mining these continuous data streams brings unique opportunities, but also new challenges. this paper describes and evaluates vfdt, an anytime system that builds decision trees using constant memory and constant time per example. vfdt can incorporate tens of thousands of examples per second using off-the-shelf hardware. it uses hoeffding bounds to guarantee that its output is asymptotically nearly identical to that of a conventional learner. we study vfdt\u2019s properties and demonstrate its utility through an extensive set of experiments on synthetic data. we apply vfdt to mining the continuous stream of web access data .", "simplified_prediction": "Many organizations today have more than very large databases ; they have databases that grow without limit at a rate of several million records per day . A lot of people bring them unique opportunities , but also new challenges . This paper describes the system that builds decision trees using constant memory and constant time per example ( see below ) . Vfdt can have tens of thousands of examples per second using hardware per second . It uses very small bounds to guarantee that its output is asymptotically nearly identical to that of a conventional learner . we study making things very important and demonstrate its utility through an extensive set of experiments on synthetic data . we apply to mining in the continuous stream of web access data ."}
{"index": 115, "ground_truth": "the paper looks at the problem of learning structured exploration policies for training rl agents. structured exploration consider a stochastic, parameterized policy \u03c0\u03b8(a|s) where \u03b8 represents the policy-parameters. to encourage exploration, noise can be added to the policy at each time step t. but the noise added in such a manner does not have any notion of temporal coherence. another issue is that if the policy is represented by a simple distribution (say parameterized unimodal gaussian), it can not model complex time-correlated stochastic processes. the paper proposes to condition the policy on per-episode random variables (z) which are sampled", "prediction": "novel reward-learning-from-observation (t-rex), trajectory-ranked reward functions from a set of potentially poor demonstrations. when combined with deep reinforcement learning, t-rex outperforms state-of-the-art imitation learning and irl methods on multiple atari and mujoco benchmark tasks and achieves performance that is often more than twice the performance of the best demonstration. we also demonstrate that t-rex is robust to ranking noise and can accurately extrapolate intention by simply watching a learner noisily improve at a task over time. due to advantages .", "simplified_prediction": "reward-learning-from-observation ( t-rex ) , trajectory-ranked reward functions from a set of potentially poor demonstrations , for example . When combined with deep reinforcement learning , the state-of-the-art imitation learning and irl methods get more than twice the best demonstration . This is done by many atari and mujoco benchmark tasks and achieves performance . we also demonstrate that t-rex is robust to ranking noise and can improve them by simply watching a learner using noisily improve at a task over time . due to advantages ."}
{"index": 116, "ground_truth": "the paper presents a framework that uses diverse suboptimal world models that can be used to break complex policies into simpler and modular sub-policies. given a task, both the sub-policies and the controller are simultaneously learned in a bottom-up manner. the framework is called as model primitive hierarchical reinforcement learning (mphrl). idea instead of learning a single transition model of the environment (aka world model) that can model the transitions very well, it is sufficient to learn several (say k) suboptimal models (aka model primitives). each model primitive will be good in only a small part of the state", "prediction": "learning techniques enforce this decomposition in a top-down manner, while meta-learning techniques require a task distribution at hand to learn such decompositions. this paper presents a framework for using diverse suboptimal world models to understand the importance and robustness of different elements in the framework and limitations to this approach. in the lifelong learning setting, we want our agent to solve a series of related tasks after learning to solve new tasks in a sequential learning setting. our work takes a step towards solutions for such incremental settings. .", "simplified_prediction": "learning techniques require a task distribution to learn such as decompositions , and meta-learning techniques require a task distribution at hand to learn such things . This paper presents a framework for using different world models to understand the importance and robustness of different elements in the framework and limitations to this approach . In the lifelong learning setting , we want our agent to solve a series of related tasks after learning to solve new tasks in a learning setting . our work takes a step towards solutions such as settings . Other pages"}
{"index": 117, "ground_truth": "the paper proposes an adversarial approach for estimating generative models where one model (generative model) tries to learn a data distribution and another model (discriminative model) tries to distinguish between samples from the generative model and original data distribution. adversarial net two models - generative model(g) and discriminative model(d) both are multi-layer perceptrons. g takes as input a noise variable z and outputs data sample x(=g(z)). d takes as input a data sample x and predicts whether it came from true data or from g. g tries to minimise log(1-d(g(z))) while d tries to maximise the probability", "prediction": "new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model g that estimates the probability that a sample came from the training data rather than g. the training procedure for g is to maximize the probability of d making a mistake. this framework corresponds to a minimax two-player in the space of arbitrary functions g and d, a unique solution exists, with g recovering the training data distribution and d equal to 1 2 everywhere. in the case where g and d are defined by multilayer perceptrons, the entire system can be trained with backpropagation. to detect the counterfeit currency. competition .", "simplified_prediction": "In which we simultaneously train two models : the probability that a sample came from the training data rather than g , the training procedure for g is to maximize the probability of making a mistake . A sample came from the training data rather than g , and the training procedure for g is to change the probability of d making a mistake . The framework corresponds to two-player in the space of arbitrary functions g and d , a unique solution exists , with g recovering the training data distribution and d equal to 1 2 everywhere . In the case where g and d are defined by the entire system , the entire system can be trained with a backpropagation . to detect the currency . The competition"}
{"index": 118, "ground_truth": "an empirical analysis of anonymity in zcash kappos et al., usenix security\u201918 as we\u2019ve seen before, in practice bitcoin offers little in the way of anonymity . zcash on the other hand was carefully designed with privacy in mind. it offers strong theoretical guarantees concerning privacy. so in theory users of zcash can remain anonymous. in practice though it depends on the way those users interact with zcash. today\u2019s paper choice, \u2018an empirical analysis of anonymity in zcash\u2019 studies how identifiable transaction participants are in practice based on the 2,242,847 transactions in the blockchain at the time of the", "prediction": "derived from bitcoin, zcash is often touted as the one with the strongest anonymity guarantees, due to its basis in well-regarded cryptographic research. in this paper, we investigate all facets of anonymity in zcash\u2019s transactions, ranging from its transparent transactions to the interactions with and within its main privacy feature, a shielded pool that acts as the anonymity set for users wishing to spend coins privately. we conclude that while it is possible to use zcash in a private way, it is also possible to shrink its anonymity set considerably by developing simple heuristics based on identifiable patterns of usage. since the introduction of bitcoin in 2008 [34], cryptocurrencies have become increasingly popular to the point of reaching a near-mania, with thousands of deployed cryptocurrencies now collectively attracting trillions of dollars in investment. .", "simplified_prediction": "from bitcoin , zcash is often used as the one with the strongest guarantees , because of its basis in well-regarded cryptographic research . In this paper , we look like all facets of anonymity in zcash Ders transactions , ranging from its transparent transactions to the interactions with and within its main feature , a shielded pool that acts as a set for users wishing to spend coins privately . we conclude that while it is possible to use zcash in a private way , it is also possible to shrink it by developing simple patterns that are not true . since the introduction of bitcoin in 2008 [ 34 ] , cryptocurrencies have become increasingly popular to the point of reaching a near-mania , with thousands of people being able to attract trillions of dollars in investment . Other pages"}
{"index": 119, "ground_truth": "statiscal foundations of virtual democracy kahng et al., icml\u201919 this is another paper on the theme of combining information and making decisions in the face of noise and uncertainty \u2013 but the setting is quite different to those we\u2019ve been looking at recently. consider a food bank that receives donations of food and distributes it to those in need. the goal is to implement an automated decision making system such that when a food donation is received, the system outputs the organisation (e.g. housing authority or food pantry) that should receive it. we could hard code a set of", "prediction": "virtual democracy is an approach to automating by learning models of the preferences of individual people, and, at runtime, aggregating the predicted preferences of those people on the dilemma at hand. one of the key questions is which aggregation or voting rule \u2014 to use; we offer a novel statistical viewpoint that provides guidance. specifically, we seek voting rules that are robust to prediction errors, in that their output on people\u2019s true preferences is likely to coincide with their output on noisy estimates thereof. we prove that the classic borda count rule is robust in this sense, whereas any voting rule belonging to the wide family of pairwisemajority consistent rules is not. because people can express a sensible opinion about a new dilemma arises. but what if we could predict the preferences of voters \u2014 instead of explicitly asking them each time \u2014 and then aggregate those predicted preferences to arrive at a decision? this paradigm is exactly the idea behind the work of noothigattu ethical decisions. .", "simplified_prediction": "democracy is an approach to find out about learning models of individual people , and , at runtime , making the predicted preferences of those people on the dilemma at hand . one of the key questions is which aggregation or voting rule by someone to use . We offer a book that does not want to do this . For this reason , we seek voting rules that are robust to prediction errors , in that their output on people own , they are likely to happen with their output on noisy estimates thereof . we prove that the borda count rule is robust in this sense , whereas any voting rule belonging to the wide family of pairwisemajority is not . because people can express a opinion about a new person 's dilemma . but what if we could predict the preferences of voters , instead of explicitly asking them each time common and then they would then have to go to a decision ? This paradigm is exactly the idea behind the work of ethical decisions . Other pages"}
