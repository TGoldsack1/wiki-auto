{"index": 100, "ground_truth": "beat: asynchronous bft made practical duan et al., ccs\u201918 reaching agreement (consensus) is hard enough, doing it in the presence of active adversaries who can tamper with or destroy your communications is much harder still. that\u2019s the world of byzantine fault tolerance (bft). we\u2019ve looked at practical bft (pbft) and honeybadger on previous editions of the morning paper. today\u2019s paper, beat, builds on top of honeybadger to offer bft with even better latency and throughput. asynchronous bft protocols are arguably the most appropriate solutions for building high-assurance and intrusion-tolerant permissioned blockchains in wide-are (wan) environments, as these asynchronous protocols", "prediction": "we present beat, a set of practical byzantine for completely asynchronous environments. beat is flexible, versatile, and extensible, consisting of five asynchronous bft designed to are designed different performance metrics, different scenarios). due to modularity in its design, of these protocols can be mixed to achieve even more meaningful trade-offs functionality and performance for various applications. through a 92-instance, five-continent deployment of beat on amazon ec2, we show that beat beat is efficient: roughly, all our beat instances significantly outperform, in terms of both latency and throughput, honeybadgerbft, honeybadgerbft, and the most efficient asynchronous bft known. machine", "simplified_prediction": "we present beat , a set of practical environments for completely asynchronous environments . beat flexible , versatile , and extensible , which is made of five bft . These are designed to be different , different scenarios . This is because modularity in its design , of these protocols can be mixed to achieve even more meaningful trade in different applications . In terms of both latency and throughput , we show that beat is efficient : roughly , all our beat instances significantly outperform , in terms of both latency and throughput , honeybadgerbft , honeybadgerbft , and the most efficient asynchronous bft known . Other pages"}
{"index": 101, "ground_truth": "this paper presents an integrated behavioral anticipation and decision-making system that models behavior for both our vehicle and nearby vehicles as the result of closed-loop policies. only a finite set of a priori known policies are considered. bayesian changepoint detection is used to estimate which policy a given vehicle was executing at each point in its history of actions, then infer the likelihood of each potential intention of the vehicle. a statistical test is proposed based on changepoint detection to identify anomalous behavior of other vehicles, such as driving in the wrong direction or swerving out of lanes. evidence", "prediction": "to operate reliably in real-world traffic, an autonomous car must evaluate the consequences of its potential actions by anticipating the uncertain intentions of other traffic participants. this paper presents an integrated behavioral inference and decision-making approach that models vehicle for both our vehicle and nearby vehicles as a discrete set of closedloop policies that react to the actions of other agents. each policy a distinct high-level behavior and intention, such as driving along a lane or turning at an intersection. we first employ bayesian changepoint detection on the observed history of states states states states of nearby to estimate to estimate to estimate estimate the each", "simplified_prediction": "In real-world traffic , an autonomous car must evaluate the consequences of its potential actions by anticipating the uncertain intentions of other traffic participants . This paper presents an integrated behavioral behavioral inference and decision-making approach that models vehicle for both our vehicle and nearby vehicles as a set of closedloop policies that make the actions of other agents . each policy a different behavior and intention , such as driving along the lane or turning at an intersection . we first use bayesian changepoint on the observed history of states states states states that are nearby to estimate that the state has a population ."}
{"index": 102, "ground_truth": "the paper presents the task of abductive nlp (pronounced as alpha nlp) where the model needs to perform abductive reasoning. abductive reasoning is the inference to the most plausible explanation. even though it is considered to be an important component for understanding narratives, the work in this domain is sparse. a new dataset called as abstractive reasoning in narrative text (art) consisting of 20k narrative contexts and 200k explanations is also provided. the dataset models the task as multiple-choice questions to make the evaluation process easy. task setup given a pair of observations o1 and o2 and two hypothesis", "prediction": "abductive reasoning is inference inference to the most jenny for example, if jenny in a mess when she returns from work, and remembers that she left a window open, she can hypothesize that a thief into her and caused the mess, as as as explanation. while abduction has while long been considered to be at the core of be at at the core of how people interpret and read between the lines in (hobbs et al., has been relatively little research in support of abductive abductive natural language inference and generation. we present the first", "simplified_prediction": "For example , if jenny in a mess when she returns from work , and remembers that she left a window open , she can tell that a thief into her and caused the mess , as as explanation . while long been considered to be at the core of be at the core of how people interpret and read between the lines in ( hobbs et al . ) has been relatively little research in support of abductive abductive natural language inference and generation . The first one"}
{"index": 103, "ground_truth": "`update 2015/11/23: since i first wrote this note, i became involved in the next iterations of this work, which became v2 of the arxiv manuscript. the notes below were made based on v1.` this paper considers the problem of maximum inner product search (mips). in mips, given a query $q$ and a set of inputs $x_i$, we want to find the input (or the top n inputs) with highest inner product, i.e. $argmax_i q' x_i$. recently, it was shown that a simple transformation to the query and input vectors made it possible to approximately solve mips using hashing methods", "prediction": "efficient maximum inner inner product search (mips) is an important task that has a wide wide applicability in recommendation systems and classification with a large number of classes. solutions based on hashing (lsh) as well as tree-based solutions have been investigated in the recent to perform approximate mips in sublinear this paper, we compare these to another extremely extremely simple approach for solving solving approximate mips, based on variants of the k-means clustering algorithm. we propose to train a spherical kmeans, reduced the mips problem having having to problem to a maximum cosine (mcss). experiments on two standard", "simplified_prediction": "The maximum inner product search ( mips ) is an important task that has a wide wide applicability in recommendation systems and classification with a large number of classes . The solutions based on hashing ( lsh ) as well as tree-based solutions have been investigated in the recent to perform very simple approach for solving solving mips . This is based on different kinds of k-means clustering algorithm . we propose to train a spherical kmeans , reduced the mips problem having to problems with a maximum problem . On two standard experiments"}
{"index": 104, "ground_truth": "understanding lifecycle management complexity of datacenter topologies zhang et al., nsdi\u201919 there has been plenty of interesting research on network topologies for datacenters, with clos-like tree topologies and expander based graph topologies both shown to scale using widely deployed hardware. this research tends to focus on performance properties such as throughput and latency, together with resilience to failures. important as these are, note that they\u2019re also what\u2019s right in front of you as a designer, and relatively easy to measure. the great thing about today\u2019s paper is that the authors look beneath the surface to consider the less visible", "prediction": "most recent datacenter designs have focused on performance properties such as latency and throughput. in this paper, we explore a new dimension, management complexity, which attempts to understand the complexity of deploying a topology and expanding it. by analyzing practice in lifecycle management, we devise complexity metrics for lifecycle management, and show that existing topology have low lifecycle management complexity by management complexity by some but not by others. motivated by this, we design design a new class of topologies, fatclique, that, while performance-equivalent to existing topologies, is to, or to, or better them by all our them our lifecycle management complexity metrics. over the past our", "simplified_prediction": "Most recent designs have focused on performance properties such as latency and throughput . In this paper , we look at a new size , management complexity , which tries to understand the complexity of using a topology and expanding it . In his lifecycle management , we look at complexity metrics for lifecycle management , and show that existing topology have low lifecycle management complexity by management complexity by some but not by others . we design a new class of topologies , fatclique , that , while performance-equivalent to existing topologies , is to , or better them by all our lifecycle management complexity ( management complexity metrics ) . the past our past"}
{"index": 105, "ground_truth": "kv-direct: high-performance in-memory key-value store with programmable nic li et al., sosp\u201917 we\u2019ve seen some pretty impressive in-memory datastores in past editions of the morning paper, including farm , ramcloud , and drtm . but nothing that compares with kv-direct: with 10 programmable nic cards in a commodity server, we achieve 1.22 billion kv operations per second, which is almost an order-of-magnitude improvement over existing systems, setting a new milestone for a general-purpose in-memory key-value store. check out the bottom line in this comparison table from the evaluation: ( enlarge ) in addition to sheer speed,", "prediction": "performance of in-memory in-memory key-value continues to be of great importance as modern kvs goes beyond the traditional object-caching workload and becomes a key infrastructure to main-memory in data centers. recent years have witnessed a rapid increase of network bandwidth in data centers, shifting the bottleneck of most kvs from the network to network to the cpu. rdma-capable partly alleviates the problem, but the primitives provided by rdma abstraction are rather are rather meanwhile, nics become in data centers, centers, paper, we present kv-direct, kv-direct, kv-direct, present a processing. in this paper, we present a high kvs that programmable to extend rdma primitives and", "simplified_prediction": "performance in-memory in-memory key-value continues to be of great importance as modern kvs goes beyond the traditional object-caching workload and becomes a key infrastructure to people in data centers . recent years have witnessed a rapid increase of network bandwidth in data centers . They changed the bottleneck of most kvs from the network to network to the cpu . However , the primitives provided by rdma abstraction are rather meanwhile , nics become in data centers , centers , paper , we present kv-direct , kv-direct , kv-direct , kv-direct , and present a processing . In this paper , we present a high kvs that are used to make rdma primitives and"}
{"index": 106, "ground_truth": "the paper presents a generalized framework for graph clustering (clusters of network motifs) on the basis of higher-order connectivity patterns. approach given a motif m , the framework aims to find a cluster of the set of nodes s such that nodes of s participate in many instances of m and avoid cutting instances of m (that is only a subset of nodes in instances of m appears in s). mathematically, the aim is to minimise the motif conductance metric given as cutm(s, s\u2019) / min[volm(s), volm(s\u2019)] where s\u2019 is complement of s, cutm(s, s\u2019) = number of instances", "prediction": "networks are a fundamental tool tool for understanding and modeling complex systems in physics, neuroscience, engineering, and social science. many networks are known to exhibit rich, lower-order connectivity patterns that can be captured at the level of individual nodes and edges. edges. however, organization of complex networks\u2014at the level of network subgraphs\u2014 remains largely unknown. here we develop a generalized framework for we develop a generalized framework connectivity patterns. this framework provides mathematical guarantees on the optimality clusters and scales to networks to networks with billions billions of edges. the framework a number of number of in propagation units in neuronal networks and structure in transportation structure transportation structure in", "simplified_prediction": "networks are an important tool for understanding and modeling complex systems in physics , engineering , and social science . Many networks have rich , lower-order connectivity patterns that can be captured at the level of individual nodes and edges . It is about . However , organization of complex networks changed the level of network name remains largely unknown . we develop a framework for we develop a good framework for a generalized framework . The framework provides mathematical guarantees on the optimality clusters and scales to networks with billions of edges . The framework a number of people in a number of units in the neuronal networks and structure in transportation structure transportation structure ."}
{"index": 107, "ground_truth": "they suggest a new method to train gans. they start training them at low resolution (4x4), wait until \"convergence\", then add more convolutions to the existing model to generate and discriminate higher resolutions. each new block of convolutions is slowly blended in, instead of being added from one batch to the next. combined with two new normalization techniques, they get good-looking images at up to 1024x1024 on their new celeba-hq dataset (celeba in high resolution). they also suggest a new scoring method based on the approximated wasserstein distance between real and generated image patches. according to that score, their progressive", "prediction": "we a new training methodology for generative adversarial networks. the key idea is to grow both the generator and discriminator starting from a low resolution, we layers that model layers that as training progresses. this both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented e.g., celeba images at 1024. we also propose also a simple way to simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised cifar10. we describe several implementation details that are discouraging unhealthy competition between between the generator and discriminator. finally, we between suggest suggest a new metric we suggest a metric metric we suggest", "simplified_prediction": "we a new training methodology for making adversarial network . The key idea is to grow both the generator and discriminator starting from a low resolution . This we layers that are made for training from a low resolution . This speeds the training up and greatly stabilizes it , allowing us to produce images of unprecedented e . g , which gives a record inception score of 8.80 in unsupervised cifar10 , and achieve a record inception score of 8.80 in unsupervised cifar10 . we describe several implementations that are different to unhealthy competition between the generator and discriminator . finally , we between suggest a new metric we suggest that a new metric we suggest"}
{"index": 108, "ground_truth": "in this paper, the authors proposed a method for convexifying convolutional neural networks to train them without backpropagation. furthermore, this relaxation to the convex setting allows for theoretical proofs of bounds on the generalization error. succinctly, they propose to use rkhs and the kernel trick to lift the data into a high-dimensional space that is expressive enough to capture certain nonlinear activation functions. hence, on experiments on mnist and cifar-10, they show that they can outperform smaller cnns by \u201cconvexifying\u201d them. they note that their method doesn\u2019t work with max pooling or very deep cnns with lots of bells and", "prediction": "class we describe the class of convexified convolutional neural networks (ccnns), which capture the parameter sharing of convolutional neural networks in a convex manner. by representing the nonlinear convolutional filters as vectors in a kernel hilbert space, the cnn parameters can be as a low-rank matrix, which can be relaxed to obtain a convex problem. for learning two-layer convolutional we prove that the generalization error obtained by a convexified converges to that of cnn converges to that of the cnn. for learning deeper we ccnns train ccnns in a layer-wise manner. empirically, empirically, ccnns trained performance competitive with cnns cnns", "simplified_prediction": "class we describe the class of neural networks ( ccns ) , which capture the parameter sharing of neural networks in a convex manner on the parameter . by representing the nonlinear convolutional filters as vectors in a kernel hilbert space , the parameters can be as a low-rank matrix , which can be relaxed to get a convex problem . For learning two-layer convolutional we prove that the generalization error was taken by a convexified converges to that of the cnn . learning deeper we ccns train in a layer-wise manner on the train . empirically , ccns trained performance competitively with cns cnns ."}
{"index": 109, "ground_truth": "proposes a novel, end-to-end architecture for generating short email responses. single most important benchmark of its success is that it is deployed in inbox by gmail and assists with around 10% of all mobile responses. . challenges in deploying smart reply in a user-facing product responses must always be of high quality. ensured by constructing a target response set to select responses from. the likelihood of choosing the responses must be maximised. ensured by normalising the responses and enforcing diversity. the system should not add latency to emails. ensured by using a triggering model to decide if the email", "prediction": "the of intelligent intelligent machines is one of the biggest unsolved challenges in computer in this paper, we some fundamental properties these machines should have, focusing in particular on in and learning. we discuss a simple environment that could be used to incrementally teach a machine the basics of natural-language-based communication, as a prerequisite to more complex interaction with human human users. we also present some conjectures on the sort of algorithms the machine should support in order to profitably learn from the environment. a machine capable of performing complex tasks without requiring requiring without would be in almost endeavor, almost human human endeavor, from performing", "simplified_prediction": "The intelligent machine is one of the biggest challenges in computer in this paper , and some fundamental properties of these machines should have , focusing in particular on in and learning . we talk about a simple environment that could be used to teach a machine the basics of natural-language-based communication , to more complex interaction with human users . we also have some conjectures on the sort of algorithms the machine should support in order to learn from the environment . A machine can perform complex tasks without needing without would be in almost endeavor , almost all of the task , from performing a certain way ."}
{"index": 110, "ground_truth": "build a supervised reading comprehension data set using news corpus. compare the performance of neural models and state-of-the-art natural language processing model on reading comprehension task. reading comprehension estimate conditional probability p(a|c, q), where c is a context document, q is a query related to the document, and a is the answer to that query. dataset generation use online newspapers (cnn and dailymail) and their matching summaries. parse summaries and bullet points into cloze style questions. generate corpus of document-query-answer triplets by replacing one entity at a time with a placeholder. data anonymized and randomised using coreference systems,", "prediction": "teaching teaching machines to read natural language documents remains an elusive challenge. machine reading systems can be tested on their ability to answer questions posed on the contents of documents documents that they have but until now large training and test have been missing for this type of evaluation. in this work we define a new methodology that resolves this bottleneck and provides large supervised reading comprehension data. this allows us to develop develop a class of attention based deep networks that learn to read real and answer complex questions with minimal minimal prior knowledge of questions with minimal the path from of language", "simplified_prediction": "The teaching machine is to read natural language documents which are still used . It can be tested on their ability to answer questions on the contents of documents that they have but until now large training and test have been missing for this type of evaluation . In this work we define a new methodology that shows this bottleneck and provides a large amount of reading data . This allows us to develop a class of attention based deep networks that learn to read real and answer complex questions with a very little amount of questions from the language ."}
{"index": 111, "ground_truth": "rpcvalet: ni-driven tail-aware balancing of \u00b5s-scale rpcs daglis et al., asplos\u201919 last week we learned about the [increased tail-latency sensitivity of microservices based applications with high rpc fan-outs. seer uses estimates of queue depths to mitigate latency spikes on the order of 10-100ms, in conjunction with a cluster manager. today\u2019s paper choice, rpcvalet, operates at latencies 3 orders of magnitude lower, targeting reduction in tail latency for services that themselves have service times on the order of a small number of \u00b5s (e.g., the average service time for memcached is approximately 2\u00b5s). the net result of rapid advancements in", "prediction": "modern online services come with stringent quality requirements in terms of response time tail latency. because of their their decomposition into fine-grained communicating software layers, a user request fans out into a plethora of short, faster inter-server in reaction to communication. in reaction to that need, we are witnessing a technological transition characterized by the emergence of hardware-terminated user-level (e.g., infiniband/rdma) and (e.g., architectures with network interfaces (nis). such interfaces architectures offer offer a unique opportunity for a new ni-driven for a new approach to balancing rpcs among the cores of manycore tail yielding tail latency rpcs. we introduce rpcvalet, introduce ni-driven an ni-driven \u03bcs-scale rpcs. of tail latency of major", "simplified_prediction": "modern online services come with quality requirements in terms of response time tail latency . Because of their decomposition into communicating software layers , a user request fans out into a plethora of short , faster inter-server in reaction to communication . In reaction to that need , we are witnessing a technological transition characterized by the emergence of hardware-terminated user-level ( e . g , infiniband / rdma ) and architectures with network interfaces ( nis ) . such interfaces offer a opportunity for a new ni-driven for a new approach to stop rpcs among the cores of manycore tail . This offers tail latency rpcs . we introduce rpcvalet , introduce ni-driven an ni-driven \u03bcs-scale rpcs . It is found in major"}
{"index": 112, "ground_truth": "scalable atomic visibility with ramp transactions \u2013 bailis et al. 2014 ramp transactions came up last week as part of the secret sauce in coordination avoidance in database systems that contributed to a 25x improvement on the tpc-c benchmark. so what exactly are ramp transactions and why might we need them? as soon as you partition your database across multiple servers, things start to get interesting. we\u2019d like to maintain atomic isolation \u2013 either all of a transaction\u2019s effects are visible or none are \u2013 for transactions that span partitions\u2026 the status quo for these multi-partition atomic transactions", "prediction": "databases databases can provide by by partitioning data across several several servers. servers. multi-operation transactional access is often expensive, employing coordination-intensive coordination-intensive mechanisms. accordingly, many realworld systems mechanisms that provide useful semantics for multi-partition this leads to incorrect for a large class of including secondary indexing, indexing, foreign key enforcement, and view maintenance. in this we identify a materialized this work, we identify a new model\u2014read atomic matches the requirements of these use cases by ensuring atomic visibility: either all or none of each transaction\u2019s of each transaction\u2019s updates are observed by other transactions. we present other algorithms for read that enforce that enforce", "simplified_prediction": "This database can provide by partitioning data across several different servers . server . Its transactional access is often expensive . It employs coordination-intensive coordination-intensive mechanisms . Accordingly , many realworld systems mechanisms that provide useful semantics for people to incorrect for a large class of including secondary indexing , indexing , foreign key enforcement , and view maintenance . In this we identify a work , we identify a new model number of atomic matches the requirements of these use cases by making the atomic visibility : either all or none of each transaction commemorates of each transactions updates are seen by other transactions . we present other algorithms for read that look like the algorithm ."}
{"index": 113, "ground_truth": "uncertainty propagation in data processing systems manousakis et al., socc\u201918 when i\u2019m writing an edition of the morning paper, i often imagine a conversation with a hypothetical reader sat in a coffee shop somewhere at the start of their day. there are three levels of takeaway from today\u2019s paper choice: if you\u2019re downing a quick espresso, then it\u2019s good to know that uncertainty can creep into our data in lots of different ways, and if you compute with those uncertain values as if they were precise, errors can compound quickly leading to incorrect results or false confidence. if", "prediction": "we are seeing an explosion of uncertain data\u2014i.e., data data\u2014i.e., data that is more properly by probability distributions estimated values with error rather than values\u2014from in iot, sampling-based approximate computations and machine learning algorithms. in many cases, performing computations computations on uncertain data as if it were exact leads to were exact leads to incorrect results. unfortunately, developing for uncertain data is a major challenge from both the mathematical and performance perspectives. this paper proposes and for tackling this dag-based data processing systems. we present a framework for uncertainty propagation (up) (up) propagation that allows developers effort. of dag dag to implementations of modest effort. to process uncertain with effort. to process uncertain uncertain inputs", "simplified_prediction": "we seeing an explosion of uncertain data behaviouri . e. , data data data that is more properly by probability distributions estimated values with error rather than values in iot , sampling-based approximate computations and machine learning algorithms . In many cases , performing computations computations on uncertain data as if they were exact lead to people who could not be incorrect . It develops unfortunately , developing for uncertain data is a major challenge from both the mathematical and performance perspective . In this paper , this paper proposes and for tackling this data processing systems . we present a framework for uncertainty propagation ( up ) ( up ) that allows developers to use it . dag dag to implementation of modest effort . This is known with effort . to see uncertain :"}
{"index": 114, "ground_truth": "mining high-speed data streams \u2013 domingos & hulten 2000 this paper won a \u2018test of time\u2019 award at kdd\u201915 as an \u2018outstanding paper from a past kdd conference beyond the last decade that has had an important impact on the data mining community.\u2019 here\u2019s what the test-of-time committee have to say about it: this paper proposes a decision tree learner for data streams, the hoeffding tree algorithm, which comes with the guarantee that the learned decision tree is asymptotically nearly identical to that of a non-incremental learner using infinitely many examples. this work constitutes a significant step", "prediction": "many organizations today have more than very large databases; databases; they have databases that without limit at a rate of several million records per day. these continuous data streams brings also challenges. this paper and vfdt, an anytime system that builds decision trees using constant using constant memory and per example. vfdt can incorporate tens of incorporate tens of of examples per second using off-the-shelf hardware. it uses hoeffding bounds to guarantee that its output is nearly identical to that of a conventional learner. we study study vfdt\u2019s properties and demonstrate its utility utility through an extensive set of experiments on synthetic data. we apply vfdt to mining the continuous", "simplified_prediction": "Many organizations today have more than very large databases ; databases have databases that can be found without limiting several million records per day . These continuous data streams bring also challenges . This paper and vfdt , an anytime system that builds decision trees using constant memory and per example . Vfdt can use tens of use for examples per second using off-the-shelf hardware . It uses borders to make sure that its output is nearly identical to that of a conventional learner . we study making things well , and it shows its utility utility through an extensive set of experiments on synthetic data . we use to mining the continuous"}
{"index": 115, "ground_truth": "the paper looks at the problem of learning structured exploration policies for training rl agents. structured exploration consider a stochastic, parameterized policy \u03c0\u03b8(a|s) where \u03b8 represents the policy-parameters. to encourage exploration, noise can be added to the policy at each time step t. but the noise added in such a manner does not have any notion of temporal coherence. another issue is that if the policy is represented by a simple distribution (say parameterized unimodal gaussian), it can not model complex time-correlated stochastic processes. the paper proposes to condition the policy on per-episode random variables (z) which are sampled", "prediction": "a critical of existing inverse inverse reinforcement learning (irl) methods is their inability outperform outperform the demonstrator. this is because irl typically seeks a reward function that makes the demonstrator near-optimal, rather than inferring the underlying intentions of the demonstrator that may have been practice. in this paper, we introduce a novel reward-learning-from-observation this paper, we introduce a novel reward-learning-from-observation reward extrapolation that extrapolates beyond a set of (approximately) ranked demonstrations in order to infer high-quality reward functions from a set of potentially poor when combined with reinforcement learning, t-rex outperforms state-of-the-art imitation learning and irl methods on multiple atari tasks and achieves performance that and achieves performance", "simplified_prediction": "a critical of existing inverse reinforcement learning ( irl ) methods is to show how good the demonstrator is . This is because irl usually seeks a reward function that makes the demonstrator near-optimal , rather than inferring the underlying of the demonstrator that may have been practice . In this paper , we introduce a novel reward-learning-from-observation this paper , we introduce a novel reward-learning-from-observation reward that extrapolates beyond a set of ( approximately ) ranked demonstrations in order to get a reward-from-observation reward functions from a set of potentially poor when combined with reinforcement learning , t-rex outperforms state-of-the-s to get to learn to learn a few things ."}
{"index": 116, "ground_truth": "the paper presents a framework that uses diverse suboptimal world models that can be used to break complex policies into simpler and modular sub-policies. given a task, both the sub-policies and the controller are simultaneously learned in a bottom-up manner. the framework is called as model primitive hierarchical reinforcement learning (mphrl). idea instead of learning a single transition model of the environment (aka world model) that can model the transitions very well, it is sufficient to learn several (say k) suboptimal models (aka model primitives). each model primitive will be good in only a small part of the state", "prediction": "learning interpretable and transferable and transferable subpolicies and performing single, complex task is difficult. some traditional hierarchical reinforcement techniques enforce this decomposition in a top-down while meta-learning techniques a task distribution at hand to learn such decompositions. this paper presents a framework for using diverse suboptimal models to decompose complex task solutions into simpler modular subpolicies. this framework performs automatic decomposition of a single source task in a bottom up manner, concurrently learning the required modular learning as as a controller to coordinate we perform perform a series of experiments on high dimensional continuous action tasks to demonstrate the effectiveness of this", "simplified_prediction": "learningable and transferable and transferable tasks and performing single , complex tasks are difficult . Some traditional hierarchical techniques use this decomposition in a top-down and meta-learning techniques a task distribution to learn such things such as decompositions . This paper presents a framework for using different ways to decompose complex task solutions into simpler modular subpolicies . This framework performs automatic decomposition of a single source task in a bottom up manner , concurrently learning the required modular learning as a controller to coordinate we perform a series of experiments on high-dimensional action tasks to show the effectiveness of this ."}
{"index": 117, "ground_truth": "the paper proposes an adversarial approach for estimating generative models where one model (generative model) tries to learn a data distribution and another model (discriminative model) tries to distinguish between samples from the generative model and original data distribution. adversarial net two models - generative model(g) and discriminative model(d) both are multi-layer perceptrons. g takes as input a noise variable z and outputs data sample x(=g(z)). d takes as input a data sample x and predicts whether it came from true data or from g. g tries to minimise log(1-d(g(z))) while d tries to maximise the probability", "prediction": "we a new framework for estimating generative models via an adversarial process, process, in which we simultaneously train two generative model g that captures the data distribution, and a discriminative model that estimates the probability that a sample came from the training data rather than g. the training procedure for g is to maximize the probability of making a mistake. this framework corresponds a mistake. a minimax game. in the space of arbitrary functions g and d, a unique solution exists, with g recovering the training data distribution and equal to 1 in to in the case where g and by multilayer defined", "simplified_prediction": "We a new framework for estimating generative models via an adversarial process , process , in which we simultaneously train two generative model g that captures the data distribution , and a model that estimates the probability that a sample came from the training data rather than the training procedure for g is to change the probability of making a mistake . This corresponds to a mistake . A Game game . In the space of arbitrary functions g and d , a solution exists , with g recovering the training data distribution and equal to 1 in the case where g and by multilayer are defined ."}
{"index": 118, "ground_truth": "an empirical analysis of anonymity in zcash kappos et al., usenix security\u201918 as we\u2019ve seen before, in practice bitcoin offers little in the way of anonymity . zcash on the other hand was carefully designed with privacy in mind. it offers strong theoretical guarantees concerning privacy. so in theory users of zcash can remain anonymous. in practice though it depends on the way those users interact with zcash. today\u2019s paper choice, \u2018an empirical analysis of anonymity in zcash\u2019 studies how identifiable transaction participants are in practice based on the 2,242,847 transactions in the blockchain at the time of the", "prediction": "among among the numerous alternative cryptocurrencies derived from bitcoin, zcash is often often touted as the one with the anonymity due to its well-regarded in this paper, we examine the extent to which anonymity is achieved in the anonymity is achieved in the deployed version of zcash. we investigate all facets of anonymity in zcash\u2019s from its transparent transactions to the interactions with and within its its privacy feature, feature, a shielded pool a shielded pool that as the anonymity set for users wishing to spend privately. we conclude that while while it is that in possible to in a private way, it", "simplified_prediction": "among the numerous alternative cryptocurrencies derived from bitcoin , zcash is often often touted as the one with the anonymity due to its well-regarded in this paper , we look at the extent to which the anonymity is achieved in the sent version of zcash . We investigate all facets of anonymity , which is a shielded pool that was a set for users wishing to spend privately . This was because of a shielded pool that as a set for users wishing to spend privately . Because of this , it is that in possible to in a private way , it"}
{"index": 119, "ground_truth": "statiscal foundations of virtual democracy kahng et al., icml\u201919 this is another paper on the theme of combining information and making decisions in the face of noise and uncertainty \u2013 but the setting is quite different to those we\u2019ve been looking at recently. consider a food bank that receives donations of food and distributes it to those in need. the goal is to implement an automated decision making system such that when a food donation is received, the system outputs the organisation (e.g. housing authority or food pantry) that should receive it. we could hard code a set of", "prediction": "virtual virtual democracy is an approach to automating decisions, by learning models of the preferences of individual people, and, at runtime, aggregating the predicted preferences of those people on the dilemma at hand. one of the key questions is which aggregation method or voting rule \u2014 to use; we offer offer a novel statistical viewpoint that provides guidance. we seek voting rules that are robust to prediction errors, in that their output on people\u2019s true preferences is likely to coincide with output on noisy estimates thereof. we prove that the classic borda count rule is robust in this whereas any voting rule belonging to the wide family of pairwisemajority is", "simplified_prediction": "virtual democracy is an approach to automated decisions , by learning models of the preferences of individual people , and , at runtime , making the predicted preferences of those people on the dilemma at hand . one of the key questions is which aggregation method or voting rule name to use . We offer a book that gives people to do this . we seek voting rules that are able to prediction errors , in that their output on people list , is likely to be the same as output on noisy estimates thereof . we prove that the borda count rule is robust in this whereas any voting rule belonged to the family of people who lived there ."}
